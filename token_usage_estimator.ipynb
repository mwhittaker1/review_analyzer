{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a79c3e",
   "metadata": {},
   "source": [
    "# Token Usage Estimator for Review Analysis\n",
    "\n",
    "This notebook estimates the token usage and cost for analyzing a large dataset of 50,000 comments by:\n",
    "1. Processing a small sample of comments using different OpenAI models\n",
    "2. Tracking token usage for both input and output\n",
    "3. Calculating average tokens per comment\n",
    "4. Projecting the total cost for analyzing 50,000 comments\n",
    "\n",
    "This approach allows for accurate cost planning without processing the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773f0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from typing import Optional, Tuple, Dict, List, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb2679",
   "metadata": {},
   "source": [
    "## Token Counting Utilities\n",
    "\n",
    "The following functions will help us count tokens and estimate costs for different OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c4cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, model: str = \"gpt-4\") -> int:\n",
    "    \"\"\"Count the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        # Try direct model mapping first\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # For models not directly supported, use cl100k_base encoding (used by gpt-4 and newer models)\n",
    "        print(f\"Model {model} not directly supported by tiktoken, using cl100k_base encoding instead\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def estimate_cost(input_tokens: int, output_tokens: int, model: str) -> float:\n",
    "    \"\"\"Estimate the cost based on input and output tokens for different models.\"\"\"\n",
    "    # Pricing per 1K tokens (as of July 2025)\n",
    "    model_pricing = {\n",
    "        \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},\n",
    "        \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
    "        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "        \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"gpt-4-mini\": {\"input\": 0.0015, \"output\": 0.0060},\n",
    "        \"gpt-4.1-nano\": {\"input\": 0.0015, \"output\": 0.0060},\n",
    "    }\n",
    "    \n",
    "    # Default to gpt-4 pricing if model not found\n",
    "    pricing = model_pricing.get(model, model_pricing[\"gpt-4\"])\n",
    "    \n",
    "    cost = (input_tokens / 1000 * pricing[\"input\"]) + (output_tokens / 1000 * pricing[\"output\"])\n",
    "    return cost\n",
    "\n",
    "def ai_analyze_comments_with_tokens(client, prompt: str, df: pd.DataFrame, model: str, debug: bool = True) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Enhanced version of ai_analyze_comments that also tracks token usage.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing the model's response and a dictionary with token counts and estimated cost\n",
    "    \"\"\"\n",
    "    df_json = df.to_json(orient=\"records\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Processing {len(df)} comments with model {model}\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are an expert linguistic analyst specializing in extracting and scoring themes from customer return comments. \"\n",
    "            \"You always return your output as a single JSON array of objects, one per input record, using exactly the keys and structure specified in the user's instructions. \"\n",
    "            \"Do not include any explanations, extra text, or formatting outside the required JSON array. \"\n",
    "            \"Be precise, consistent, and strictly follow the output schema and scoring rules provided.\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": df_json}\n",
    "    ]\n",
    "    \n",
    "    # Count input tokens\n",
    "    input_text = \"\".join([msg[\"content\"] for msg in messages])\n",
    "    input_tokens = count_tokens(input_text, model)\n",
    "    \n",
    "    # Make API call\n",
    "    start_time = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    content = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    # Count output tokens\n",
    "    output_tokens = count_tokens(content, model)\n",
    "    \n",
    "    # Calculate usage stats\n",
    "    usage_stats = {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": input_tokens + output_tokens,\n",
    "        \"cost\": estimate_cost(input_tokens, output_tokens, model),\n",
    "        \"time\": elapsed_time,\n",
    "        \"tokens_per_comment\": (input_tokens + output_tokens) / len(df)\n",
    "    }\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Token usage stats:\")\n",
    "        print(f\"  Input tokens: {input_tokens:,}\")\n",
    "        print(f\"  Output tokens: {output_tokens:,}\")\n",
    "        print(f\"  Total tokens: {input_tokens + output_tokens:,}\")\n",
    "        print(f\"  Cost: ${usage_stats['cost']:.4f}\")\n",
    "        print(f\"  Time: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"  Tokens per comment: {usage_stats['tokens_per_comment']:.1f}\")\n",
    "    \n",
    "    return content, usage_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027f1a2",
   "metadata": {},
   "source": [
    "## 1. Fetch a Small Sample of Comments\n",
    "\n",
    "We'll use the existing functions to load data from the database and fetch a sample of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e4c8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to existing database at return_coomment_group\n",
      "Fetched 200 sample comments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RETURN COMMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I loved the dress and the fit I just dont have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The straps are too long on this and the medium.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irritated my skin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Its not a good fit for me and the material was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      RETURN COMMENT\n",
       "0  I loved the dress and the fit I just dont have...\n",
       "1                                              Tight\n",
       "2    The straps are too long on this and the medium.\n",
       "3                                  Irritated my skin\n",
       "4  Its not a good fit for me and the material was..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database connection and import functions\n",
    "def fetch_return_comments(con, tname, is_sample=True, sample_size=100, comment_only=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch return comments from the DuckDB table.\n",
    "    If is_sample is True, fetch a sample of 'sample_size' rows.\n",
    "    If comment_only is True, only returns the RETURN_COMMENT column.\n",
    "    If comment_only is False, returns all columns.\n",
    "    \"\"\"\n",
    "    if is_sample: \n",
    "        sample_query = f\"ORDER BY RANDOM() LIMIT {sample_size}\"\n",
    "    else:\n",
    "        sample_query = \"\"\n",
    "        \n",
    "    if comment_only:\n",
    "        query = f\"\"\"\n",
    "        SELECT \"RETURN COMMENT\"\n",
    "        FROM {tname}\n",
    "        {sample_query}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {tname}\n",
    "        {sample_query}\n",
    "        \"\"\"\n",
    "    \n",
    "    return con.execute(query).df()\n",
    "\n",
    "# Connect to the database\n",
    "db_path = 'return_coomment_group'\n",
    "tname = 'staging_table'\n",
    "\n",
    "# If database doesn't exist yet, import the data\n",
    "file_exists = os.path.exists(db_path)\n",
    "if not file_exists:\n",
    "    file_path = r'data\\RETURN_COMMENTS_GROUP.xlsx'\n",
    "    print(f\"Database not found. Importing data from {file_path}...\")\n",
    "    \n",
    "    # Define import function\n",
    "    def import_data(fname, clear=True, db_path='temp_db', tname='staging_table', ftype=None):\n",
    "        import os\n",
    "        con = duckdb.connect(db_path)\n",
    "        if ftype is None:\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext == '.csv':\n",
    "                ftype = 'csv'\n",
    "            elif ext == '.parquet':\n",
    "                ftype = 'parquet'\n",
    "            elif ext in ('.xlsx', '.xls'):\n",
    "                ftype = 'excel'\n",
    "            else:\n",
    "                con.close()\n",
    "                raise ValueError(\"Unsupported file extension.\")\n",
    "        if ftype == 'excel':\n",
    "            df = pd.read_excel(fname)\n",
    "            if clear:\n",
    "                con.execute(f\"DROP TABLE IF EXISTS {tname}\")\n",
    "            con.register('temp_excel_df', df)\n",
    "            con.execute(f\"CREATE TABLE {tname} AS SELECT * FROM temp_excel_df\")\n",
    "            con.unregister('temp_excel_df')\n",
    "        return con\n",
    "    \n",
    "    con = import_data(file_path, db_path=db_path, tname=tname, clear=True)\n",
    "    print(f\"Import completed.\")\n",
    "else:\n",
    "    con = duckdb.connect(db_path)\n",
    "    print(f\"Connected to existing database at {db_path}\")\n",
    "\n",
    "# Fetch a small sample of comments for testing (change sample_size as needed)\n",
    "con = duckdb.connect(db_path)\n",
    "sample_size = 200\n",
    "df_sample = fetch_return_comments(con, tname, is_sample=True, sample_size=sample_size, comment_only=True)\n",
    "print(f\"Fetched {len(df_sample)} sample comments\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd36b5f",
   "metadata": {},
   "source": [
    "## 2. Set up OpenAI Client\n",
    "\n",
    "We need to set up the OpenAI client with an API key. You can use the existing functions to access the API key from Google Secret Manager or set it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec5d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key retrieved from Google Secret Manager\n",
      "Loaded customer prompt from prompts/customer_sentiment_prompt.txt\n",
      "Loaded product prompt from prompts/product_prompt.txt\n",
      "Could not load function prompt from v1/function_calling_prompt.txt: [Errno 2] No such file or directory: 'v1/function_calling_prompt.txt'\n",
      "Using 'customer' prompt\n"
     ]
    }
   ],
   "source": [
    "# Set up OpenAI client (choose one of the options below)\n",
    "\n",
    "# Option 1: Use the access_secret function (uncomment if using Google Secret Manager)\n",
    "try:\n",
    "    from google.cloud import secretmanager\n",
    "    def access_secret(secret_path):\n",
    "        \"\"\"Establishes connection to GCP secret manager and retrieves secret value.\"\"\"\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "        response = client.access_secret_version(name=secret_path)\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "        return secret_payload\n",
    "    \n",
    "    # Replace with your secret path\n",
    "    secret_path = \"projects/572292574132/secrets/openai_monday_status_alerts/versions/latest\"\n",
    "    api_key = access_secret(secret_path)\n",
    "    print(\"API key retrieved from Google Secret Manager\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve API key from Google Secret Manager: {e}\")\n",
    "    # Fall back to environment variable\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        api_key = input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Option 2: Use environment variable (uncomment if not using Google Secret Manager)\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not api_key:\n",
    "#     api_key = input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Load prompts\n",
    "prompt_paths = {\n",
    "    'customer': 'prompts/customer_sentiment_prompt.txt',\n",
    "    'product': 'prompts/product_prompt.txt',\n",
    "    'function': 'v1/function_calling_prompt.txt'\n",
    "}\n",
    "\n",
    "prompts = {}\n",
    "for key, path in prompt_paths.items():\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            prompts[key] = f.read()\n",
    "        print(f\"Loaded {key} prompt from {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {key} prompt from {path}: {e}\")\n",
    "        prompts[key] = None\n",
    "\n",
    "# Choose which prompt to use for the test\n",
    "active_prompt_key = 'customer'  # Change to 'customer' or 'product' as needed\n",
    "active_prompt = prompts[active_prompt_key]\n",
    "\n",
    "print(f\"Using '{active_prompt_key}' prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17731871",
   "metadata": {},
   "source": [
    "## 3. Run Sentiment Analysis on Sample with Token Tracking\n",
    "\n",
    "Now we'll run the sentiment analysis on our sample comments and track token usage for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a381b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Testing model: gpt-3.5-turbo ====\n",
      "Processing 200 comments with model gpt-3.5-turbo\n",
      "Token usage stats:\n",
      "  Input tokens: 3,823\n",
      "  Output tokens: 4,095\n",
      "  Total tokens: 7,918\n",
      "  Cost: $0.0081\n",
      "  Time: 35.87 seconds\n",
      "  Tokens per comment: 39.6\n",
      "Error parsing JSON response from gpt-3.5-turbo\n",
      "Raw response: [\n",
      "    {\n",
      "        \"IDENTIFIER\": \"C1\",\n",
      "        \"RETURN_NO\": \"1\",\n",
      "        \"RETURN_COMMENT\": \"I loved the dress and the fit I just dont have anywhere to where it\",\n",
      "        \"Theme 1\": \"Loved the dress\",\n",
      "        \"Sentiment 1\": 4,\n",
      "        \"Theme 2\": \"Good fit\",\n",
      "        \"Sentiment 2\": 4,\n",
      "        \"Theme 3\": \"\",\n",
      "        \"Sentiment 3\": 0,\n",
      "        \"Theme 4\": \"\",\n",
      "        \"Sentiment 4\": 0,\n",
      "        \"Pos_mean\": 4.0,\n",
      "        \"Neg_mean\": 0.0,\n",
      "        \"Total_sentiment\": 4.0\n",
      "    },\n",
      "    {\n",
      "        \"IDENTIFIER\": \"C2\",\n",
      "...\n",
      "\n",
      "==== Testing model: gpt-4o ====\n",
      "Processing 200 comments with model gpt-4o\n",
      "Token usage stats:\n",
      "  Input tokens: 3,791\n",
      "  Output tokens: 16,384\n",
      "  Total tokens: 20,175\n",
      "  Cost: $0.2647\n",
      "  Time: 318.94 seconds\n",
      "  Tokens per comment: 100.9\n",
      "Error parsing JSON response from gpt-4o\n",
      "Raw response: ```json\n",
      "[\n",
      "  {\n",
      "    \"IDENTIFIER\": \"C1\",\n",
      "    \"RETURN_NO\": \"\",\n",
      "    \"RETURN_COMMENT\": \"I loved the dress and the fit I just dont have anywhere to where it\",\n",
      "    \"Theme 1\": \"Loved the dress\",\n",
      "    \"Sentiment 1\": 5,\n",
      "    \"Theme 2\": \"Loved the fit\",\n",
      "    \"Sentiment 2\": 5,\n",
      "    \"Theme 3\": \"\",\n",
      "    \"Sentiment 3\": 0,\n",
      "    \"Theme 4\": \"\",\n",
      "    \"Sentiment 4\": 0,\n",
      "    \"Pos_mean\": 5.0,\n",
      "    \"Neg_mean\": 0.0,\n",
      "    \"Total_sentiment\": 5.0\n",
      "  },\n",
      "  {\n",
      "    \"IDENTIFIER\": \"C2\",\n",
      "    \"RETURN_NO\": \"\",\n",
      "    \"RETURN_COMMENT\": \"Tight\",\n",
      "  ...\n",
      "\n",
      "==== Testing model: gpt-4-turbo ====\n",
      "Processing 200 comments with model gpt-4-turbo\n",
      "Token usage stats:\n",
      "  Input tokens: 3,823\n",
      "  Output tokens: 1,403\n",
      "  Total tokens: 5,226\n",
      "  Cost: $0.0803\n",
      "  Time: 46.81 seconds\n",
      "  Tokens per comment: 26.1\n",
      "Successfully processed 10 comments with gpt-4-turbo\n",
      "\n",
      "==== Testing model: gpt-4.1-nano ====\n",
      "Processing 200 comments with model gpt-4.1-nano\n",
      "Model gpt-4.1-nano not directly supported by tiktoken, using cl100k_base encoding instead\n",
      "Model gpt-4.1-nano not directly supported by tiktoken, using cl100k_base encoding instead\n",
      "Token usage stats:\n",
      "  Input tokens: 3,823\n",
      "  Output tokens: 18,671\n",
      "  Total tokens: 22,494\n",
      "  Cost: $0.1178\n",
      "  Time: 157.78 seconds\n",
      "  Tokens per comment: 112.5\n",
      "Successfully processed 131 comments with gpt-4.1-nano\n",
      "\n",
      "==== Model Comparison Summary ====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Tokens</th>\n",
       "      <th>Output Tokens</th>\n",
       "      <th>Total Tokens</th>\n",
       "      <th>Tokens/Comment</th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Cost ($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3823</td>\n",
       "      <td>4095</td>\n",
       "      <td>7918</td>\n",
       "      <td>39.590</td>\n",
       "      <td>35.866342</td>\n",
       "      <td>0.008054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>3791</td>\n",
       "      <td>16384</td>\n",
       "      <td>20175</td>\n",
       "      <td>100.875</td>\n",
       "      <td>318.940446</td>\n",
       "      <td>0.264715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>3823</td>\n",
       "      <td>1403</td>\n",
       "      <td>5226</td>\n",
       "      <td>26.130</td>\n",
       "      <td>46.814677</td>\n",
       "      <td>0.080320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>3823</td>\n",
       "      <td>18671</td>\n",
       "      <td>22494</td>\n",
       "      <td>112.470</td>\n",
       "      <td>157.784330</td>\n",
       "      <td>0.117761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Input Tokens  Output Tokens  Total Tokens  Tokens/Comment  \\\n",
       "0  gpt-3.5-turbo          3823           4095          7918          39.590   \n",
       "1         gpt-4o          3791          16384         20175         100.875   \n",
       "2    gpt-4-turbo          3823           1403          5226          26.130   \n",
       "3   gpt-4.1-nano          3823          18671         22494         112.470   \n",
       "\n",
       "   Time (sec)  Cost ($)  \n",
       "0   35.866342  0.008054  \n",
       "1  318.940446  0.264715  \n",
       "2   46.814677  0.080320  \n",
       "3  157.784330  0.117761  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define models to test\n",
    "models_to_test = [\n",
    "    \"gpt-3.5-turbo\",     # Most affordable option\n",
    "    \"gpt-4o\",            # Best performance/price ratio\n",
    "    \"gpt-4-turbo\",        # High-quality results\n",
    "    \"gpt-4.1-nano\"\n",
    "    # Add other models you have access to\n",
    "]\n",
    "\n",
    "# Run analysis on each model and collect usage stats\n",
    "model_results = {}\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"\\n==== Testing model: {model} ====\")\n",
    "    try:\n",
    "        # Run analysis and track token usage\n",
    "        response_str, usage_stats = ai_analyze_comments_with_tokens(\n",
    "            client, \n",
    "            active_prompt, \n",
    "            df_sample, \n",
    "            model=model, \n",
    "            debug=True\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        try:\n",
    "            results = json.loads(response_str)\n",
    "            if isinstance(results, dict):\n",
    "                results = [results]\n",
    "            \n",
    "            # Store results and usage stats\n",
    "            model_results[model] = {\n",
    "                \"response\": results,\n",
    "                \"usage_stats\": usage_stats\n",
    "            }\n",
    "            \n",
    "            print(f\"Successfully processed {len(results)} comments with {model}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing JSON response from {model}\")\n",
    "            print(f\"Raw response: {response_str[:500]}...\")\n",
    "            model_results[model] = {\n",
    "                \"response\": None,\n",
    "                \"usage_stats\": usage_stats\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with {model}: {e}\")\n",
    "        model_results[model] = {\n",
    "            \"response\": None,\n",
    "            \"usage_stats\": {\"error\": str(e)}\n",
    "        }\n",
    "\n",
    "# Display a summary of all model results\n",
    "print(\"\\n==== Model Comparison Summary ====\")\n",
    "summary_data = []\n",
    "\n",
    "for model, data in model_results.items():\n",
    "    if \"usage_stats\" in data and \"error\" not in data[\"usage_stats\"]:\n",
    "        stats = data[\"usage_stats\"]\n",
    "        summary_data.append({\n",
    "            \"Model\": model,\n",
    "            \"Input Tokens\": stats[\"input_tokens\"],\n",
    "            \"Output Tokens\": stats[\"output_tokens\"],\n",
    "            \"Total Tokens\": stats[\"total_tokens\"],\n",
    "            \"Tokens/Comment\": stats[\"tokens_per_comment\"],\n",
    "            \"Time (sec)\": stats[\"time\"],\n",
    "            \"Cost ($)\": stats[\"cost\"]\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bbfb9",
   "metadata": {},
   "source": [
    "## 4. Estimate Cost for 50,000 Comments\n",
    "\n",
    "Now we'll use our sample results to estimate the cost of processing 50,000 comments with each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e44f41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Projected Input Tokens</th>\n",
       "      <th>Projected Output Tokens</th>\n",
       "      <th>Projected Total Tokens</th>\n",
       "      <th>Projected Cost ($)</th>\n",
       "      <th>Projected Time (min)</th>\n",
       "      <th>Projected Time (hours)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>955749</td>\n",
       "      <td>1023750</td>\n",
       "      <td>1979500</td>\n",
       "      <td>2.013500</td>\n",
       "      <td>149.443092</td>\n",
       "      <td>2.490718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>947749</td>\n",
       "      <td>4096000</td>\n",
       "      <td>5043750</td>\n",
       "      <td>66.178750</td>\n",
       "      <td>1328.918525</td>\n",
       "      <td>22.148642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>955749</td>\n",
       "      <td>350750</td>\n",
       "      <td>1306500</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>195.061153</td>\n",
       "      <td>3.251019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>955749</td>\n",
       "      <td>4667750</td>\n",
       "      <td>5623500</td>\n",
       "      <td>29.440125</td>\n",
       "      <td>657.434710</td>\n",
       "      <td>10.957245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Projected Input Tokens  Projected Output Tokens  \\\n",
       "0  gpt-3.5-turbo                  955749                  1023750   \n",
       "1         gpt-4o                  947749                  4096000   \n",
       "2    gpt-4-turbo                  955749                   350750   \n",
       "3   gpt-4.1-nano                  955749                  4667750   \n",
       "\n",
       "   Projected Total Tokens  Projected Cost ($)  Projected Time (min)  \\\n",
       "0                 1979500            2.013500            149.443092   \n",
       "1                 5043750           66.178750           1328.918525   \n",
       "2                 1306500           20.080000            195.061153   \n",
       "3                 5623500           29.440125            657.434710   \n",
       "\n",
       "   Projected Time (hours)  \n",
       "0                2.490718  \n",
       "1               22.148642  \n",
       "2                3.251019  \n",
       "3               10.957245  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate projections for 50,000 comments\n",
    "target_comment_count = 50000\n",
    "projection_data = []\n",
    "\n",
    "for model, data in model_results.items():\n",
    "    if \"usage_stats\" in data and \"error\" not in data[\"usage_stats\"]:\n",
    "        stats = data[\"usage_stats\"]\n",
    "        \n",
    "        # Calculate projected token usage\n",
    "        proj_input_tokens = stats[\"input_tokens\"] / len(df_sample) * target_comment_count\n",
    "        proj_output_tokens = stats[\"output_tokens\"] / len(df_sample) * target_comment_count\n",
    "        proj_total_tokens = proj_input_tokens + proj_output_tokens\n",
    "        \n",
    "        # Calculate projected cost\n",
    "        proj_cost = estimate_cost(proj_input_tokens, proj_output_tokens, model)\n",
    "        \n",
    "        # Calculate projected time (rough estimate)\n",
    "        proj_time_seconds = stats[\"time\"] / len(df_sample) * target_comment_count\n",
    "        proj_time_minutes = proj_time_seconds / 60\n",
    "        proj_time_hours = proj_time_minutes / 60\n",
    "        \n",
    "        # Add to projection data\n",
    "        projection_data.append({\n",
    "            \"Model\": model,\n",
    "            \"Projected Input Tokens\": int(proj_input_tokens),\n",
    "            \"Projected Output Tokens\": int(proj_output_tokens),\n",
    "            \"Projected Total Tokens\": int(proj_total_tokens),\n",
    "            \"Projected Cost ($)\": proj_cost,\n",
    "            \"Projected Time (min)\": proj_time_minutes,\n",
    "            \"Projected Time (hours)\": proj_time_hours\n",
    "        })\n",
    "\n",
    "# Create projection DataFrame\n",
    "projection_df = pd.DataFrame(projection_data)\n",
    "projection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79821567",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize cost projections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Cost comparison\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize cost projections\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Cost comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Model', y='Projected Cost ($)', data=projection_df)\n",
    "plt.title(f'Projected Cost for {target_comment_count:,} Comments')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Token usage comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "token_data = []\n",
    "for _, row in projection_df.iterrows():\n",
    "    token_data.append({\n",
    "        'Model': row['Model'],\n",
    "        'Tokens': row['Projected Input Tokens'],\n",
    "        'Type': 'Input'\n",
    "    })\n",
    "    token_data.append({\n",
    "        'Model': row['Model'],\n",
    "        'Tokens': row['Projected Output Tokens'],\n",
    "        'Type': 'Output'\n",
    "    })\n",
    "\n",
    "token_df = pd.DataFrame(token_data)\n",
    "sns.barplot(x='Model', y='Tokens', hue='Type', data=token_df)\n",
    "plt.title(f'Projected Token Usage for {target_comment_count:,} Comments')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create a formatted summary table\n",
    "summary_table = projection_df.copy()\n",
    "summary_table['Projected Cost ($)'] = summary_table['Projected Cost ($)'].apply(lambda x: f\"${x:,.2f}\")\n",
    "summary_table['Projected Total Tokens'] = summary_table['Projected Total Tokens'].apply(lambda x: f\"{x:,}\")\n",
    "summary_table['Projected Time (hours)'] = summary_table['Projected Time (hours)'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "print(f\"\\n==== Cost Projection Summary for {target_comment_count:,} Comments ====\")\n",
    "print(summary_table[['Model', 'Projected Total Tokens', 'Projected Cost ($)', 'Projected Time (hours)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a707da",
   "metadata": {},
   "source": [
    "## 5. Display and Export Results\n",
    "\n",
    "Let's examine the sentiment analysis results and save our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model to examine the results\n",
    "model_to_examine = models_to_test[0]  # Change index to examine different models\n",
    "\n",
    "# Display sample of analysis results\n",
    "if model_results[model_to_examine][\"response\"]:\n",
    "    results_df = pd.DataFrame(model_results[model_to_examine][\"response\"])\n",
    "    print(f\"\\n==== Sample Analysis Results from {model_to_examine} ====\")\n",
    "    display(results_df.head())\n",
    "else:\n",
    "    print(f\"No valid results available for {model_to_examine}\")\n",
    "\n",
    "# Export the results\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "export_filename = f\"token_usage_estimate_{timestamp}.xlsx\"\n",
    "\n",
    "# Create Excel writer\n",
    "with pd.ExcelWriter(export_filename) as writer:\n",
    "    # Save the model comparison summary\n",
    "    summary_df.to_excel(writer, sheet_name='Model Comparison', index=False)\n",
    "    \n",
    "    # Save the projection data\n",
    "    projection_df.to_excel(writer, sheet_name='50k Projection', index=False)\n",
    "    \n",
    "    # Save sample results for each model\n",
    "    for model in models_to_test:\n",
    "        if model_results[model][\"response\"]:\n",
    "            results_df = pd.DataFrame(model_results[model][\"response\"])\n",
    "            results_df.to_excel(writer, sheet_name=f'{model[:10]} Results', index=False)\n",
    "\n",
    "print(f\"\\nResults exported to {export_filename}\")\n",
    "\n",
    "# Close the database connection\n",
    "con.close()\n",
    "print(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5ab33",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations\n",
    "\n",
    "Based on the token usage analysis and cost projections, you can make informed decisions about:\n",
    "\n",
    "1. **Which model to use for the full 50,000 comment analysis**:\n",
    "   - Balance between cost, speed, and quality of analysis\n",
    "   - Consider gpt-4.1-nano for cost-efficiency or gpt-4o for best quality\n",
    "\n",
    "2. **Batch size optimization**:\n",
    "   - Adjust batch sizes to optimize API calls\n",
    "   - Smaller batches allow for better error recovery but more API calls\n",
    "\n",
    "3. **Budget planning**:\n",
    "   - Use the projected costs to set an appropriate budget\n",
    "   - Consider splitting the analysis across multiple days if needed\n",
    "\n",
    "4. **Error handling strategy**:\n",
    "   - Implement robust error handling for the full analysis\n",
    "   - Save progress regularly to allow for resuming after errors\n",
    "\n",
    "This notebook provides a reliable estimate of token usage and costs without having to process the entire dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
